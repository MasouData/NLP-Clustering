# âœ¨ Advanced NLP Clustering for Academic Papers

This project implements a modern NLP pipeline for clustering academic papers using state-of-the-art techniques:

* ðŸ§  **Sentence Transformers** for semantic embeddings
* ðŸ“‰ **UMAP** for dimensionality reduction
* ðŸ§© **Agglomerative Clustering** with stability validation
* ðŸ·ï¸ **Hybrid Labeling** combining **KeyBERT** and **Groq's LLM (Llama 3)**

---

## ðŸš€ Features

âœ… **Domain-Specific Enhancement**
Technical term highlighting and abbreviation expansion

âœ… **Optimal Cluster Detection**
Combined silhouette score and stability metrics

âœ… **Advanced Labeling**
Hybrid KeyBERT + LLM approach with robust output cleaning

âœ… **Efficient Processing**
Batch embedding generation and UMAP dimensionality reduction

âœ… **Reproducible Research**
Full configuration management for consistent results

---

## ðŸ› ï¸ Installation

1ï¸âƒ£ **Clone the repository**

```bash
git clone https://github.com/YOUR_USERNAME/nlp-clustering.git
cd nlp-clustering
```

2ï¸âƒ£ **Install dependencies**

```bash
pip install -r requirements.txt
```

3ï¸âƒ£ **Set up environment variables**

```bash
echo "GROQ_API_KEY=your_api_key_here" > .env
```

---

## ðŸ“ˆ Usage

### 1ï¸âƒ£ Download Data

* Download the [arXiv dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv/data)
* Place `arxiv-metadata-oai-snapshot.json` in the same directory as your Python files

---

### 2ï¸âƒ£ Preprocess Data

```bash
python preprocess_arxiv.py
```

---

### 3ï¸âƒ£ Run Clustering Pipeline

```bash
python arxiv_clustering.py
```

---

## ðŸ§¬ Workflow Overview

```mermaid
graph TD
    A[Raw arXiv Data]
    B(Preprocessing)
    C[Embedding Generation]
    D[UMAP Dimensionality Reduction]
    E[Cluster Optimization]
    F[Semantic Labeling]
    G[Results Analysis]

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
```

---

## ðŸ“Š Analyze Results (Example)

You can quickly analyze and visualize cluster distributions:

```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("clustered_arxiv.csv")

print(f"Total papers: {len(df)}")
print("Cluster distribution:\n", df["cluster"].value_counts())

df["cluster_label"].value_counts().plot.barh()
plt.title("Paper Distribution by Topic Cluster")
plt.tight_layout()
plt.savefig("cluster_distribution.png")
```

---

## ðŸŒŸ License

This project is licensed under the MIT License.

---

## ðŸ’¬ Questions?

Feel free to open an issue or reach out!

